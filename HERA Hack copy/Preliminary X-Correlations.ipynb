{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy.random as ra\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from scipy import signal\n",
    "import numpy.linalg as la \n",
    "import HERA_hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlations \n",
    "\n",
    "Here we compute the cross-correlation functions for a couple of preliminary maps. \n",
    "\n",
    "Recall: IRL the data the fields are **not isotropic** \n",
    "\n",
    "Okay with that out of the way, we can get to working on the cross-correlations. What we want to do in generate an ensemble of both noise and foreground realizaitons. Compute the dot product of the two vectors, this essensially will give you a measure of how correlated the fields are. If they both have consistenly positive values, then the result of their dot product will be positive. if they are both consitently negative then their result will be negative. If they are both randomly varied then their dot product will be averaged down near 0. \n",
    "\n",
    "Obviously one realization does not give you a full picture of the statitics because there are only a few pixels. So we will compute the dot product for many realizations. Plot the resultant dot products and perform stat analysis on them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### DEFINE OBS AND NECESSARY VARS #########\n",
    "\n",
    "\n",
    "freq_fid = 150\n",
    "\n",
    "dishes = np.array([[0,0],[0,55],[30,30],[0,60],[2,55],[47,2],[45,23],[56,21],[30,115],[48,52],[100,100],[0,200],[115,30],[33,31],[49,11],[21,24],[25,6],[56,9],[12,13],[16,17],[38,17],[60,14],[26,28],[6,45],[3,37],[12,55],[200,0],[145,13],[134,65],[139,163]])\n",
    "\n",
    "#observable corners of the sky [lat,long]\n",
    "acorner = np.array([[120,270],[122,280],[120,280],[122,270]])\n",
    "\n",
    "HERA = HERA_hack.telescope(dishes, latitude=-30, channel_width=1., Tsys=300, beam_width=3, beam = 'gaussian')\n",
    "\n",
    "obs = HERA_hack.observation(HERA, 100, 100, 0.01,acorner,1, 0.2, norm = False, pbeam = False)\n",
    "\n",
    "def generate_foregrounds():\n",
    "    ############ SYNCHRO EMISSION ############\n",
    "\n",
    "    alpha_0_syn = 2.8\n",
    "    sigma_syn = 0.1\n",
    "    Asyn = 335.4 #K\n",
    "\n",
    "    pixel_flux_syn = []\n",
    "\n",
    "    alpha_syn = np.random.normal(alpha_0_syn,sigma_syn,obs.Npix)\n",
    "\n",
    "    for i in range(obs.Npix):\n",
    "        flux = Asyn*(obs.freq/freq_fid)**(-alpha_syn[i])\n",
    "        pixel_flux_syn.append(flux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########### FREE FREE EMISSION ##########\n",
    "\n",
    "    alpha_0_ff = 2.15\n",
    "    sigma_ff = 0.01\n",
    "    Aff = 33.5 #K\n",
    "\n",
    "    pixel_flux_ff = []\n",
    "\n",
    "    alpha_ff = np.random.normal(alpha_0_ff,sigma_ff,obs.Npix)\n",
    "\n",
    "    for i in range(obs.Npix):\n",
    "        flux = Aff*(obs.freq/freq_fid)**(-alpha_ff[i])\n",
    "        pixel_flux_ff.append(flux)\n",
    "\n",
    "    ########### UNRES POINT SOURCE ###########\n",
    "\n",
    "    gamma = 1.75\n",
    "\n",
    "    def dnds(s):\n",
    "        return 4.*(s/880)**(-gamma)\n",
    "\n",
    "    s = np.arange(8,100,1) #maybe make this an argument \n",
    "    n_sources = 10\n",
    "\n",
    "    pdf = np.asarray([s,dnds(s)]) #0 is s, 1 is dnds\n",
    "    prob = pdf[1]/float(sum(pdf[1]))\n",
    "    cum_prob = np.cumsum(prob)\n",
    "\n",
    "    def gen_fluxes(N):\n",
    "        R = ra.uniform(0, 1, N)\n",
    "        #Here we first find the bin interval that random number lies in min(cum_prob[])\n",
    "        #then we find the flux who's index is that cum_prob\n",
    "        #repat for all r in R\n",
    "        return [int(s[np.argwhere(cum_prob == min(cum_prob[(cum_prob - r) > 0]))]) for r in R]\n",
    "\n",
    "    alpha_0 = 2.5\n",
    "    sigma = 0.5\n",
    "\n",
    "    theta_res = np.abs(np.cos(obs.observable_coordinates()[1,0])-np.cos(obs.observable_coordinates()[0,0]))\n",
    "    phi_res = obs.observable_coordinates()[30,1]- obs.observable_coordinates()[1,1]\n",
    "    omega_pix = theta_res*phi_res\n",
    "    factor = 1.4e-6*((obs.freq/freq_fid)**(-2))*(omega_pix**(-1))\n",
    "\n",
    "    pixel_flux = []\n",
    "\n",
    "    for i in range(obs.Npix):\n",
    "        alpha = np.random.normal(alpha_0,sigma,n_sources)\n",
    "        S_star = gen_fluxes(n_sources)\n",
    "        sum_fluxes = 0 \n",
    "\n",
    "        for i in range(n_sources-1):\n",
    "            sum_fluxes += factor*S_star[i]*(obs.freq/freq_fid)**(-alpha[i])\n",
    "\n",
    "        pixel_flux.append(sum_fluxes/n_sources)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    ########## TOTAL FG ################\n",
    "\n",
    "    pixel_flux = np.asarray(pixel_flux)\n",
    "    pixel_flux_ff = np.asarray(pixel_flux_ff)\n",
    "    pixel_flux_syn = np.asarray(pixel_flux_syn)\n",
    "\n",
    "    total_fg = pixel_flux + pixel_flux_ff + pixel_flux_syn\n",
    "    \n",
    "    \n",
    "    ##### Bright Point Sources (PHHASE SPACE) ########\n",
    "    \n",
    "    #vis_ps = cos(2*np.pi*(bdotr)) - j*sin(2*np.pi*(bdotr)) \n",
    "    #(where b is computed for each baseline and r is the pos of point source)\n",
    "    #depending on how the astro data is stored, may need to include some coord transformations\n",
    "    #COORD TRANSFORM DO NOT FORGET. \n",
    "    \n",
    "\n",
    "    return total_fg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.generate_map_noise()\n",
    "\n",
    "nreal = 10\n",
    "\n",
    "noise_1 = np.zeros((obs.Npix,nreal))\n",
    "\n",
    "noise_2 = np.zeros((obs.Npix,nreal))\n",
    "\n",
    "fg = np.zeros((obs.Npix,nreal))\n",
    "\n",
    "cov = np.zeros(nreal)\n",
    "\n",
    "mean_prods = np.zeros(nreal)\n",
    "\n",
    "sigma_prod = np.zeros(nreal)\n",
    "\n",
    "for i in range(nreal):\n",
    "    noise_1= np.real(obs.generate_map_noise())\n",
    "    noise_2= np.real(obs.generate_map_noise())\n",
    "    fg = generate_foregrounds()\n",
    "    cov[i] = np.dot(noise_1,fg)\n",
    "    mean_prods[i] = (np.mean(noise_1))*(np.mean(fg))\n",
    "    sigma_prod[i] = (statistics.stdev(noise_1))*(statistics.stdev(fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[432.09889239 157.39811834 -10.65513621 111.33392815 -67.69936561\n",
      " 170.82406484 374.45635912 583.93798564  25.25754236 -84.11364673]\n"
     ]
    }
   ],
   "source": [
    "print(cov)\n",
    "\n",
    "stacked = np.reshape((np.dstack((cov,mean_prods,sigma_prod))),(nreal,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.32098892e+02,  2.87577384e-01,  3.22683445e-02],\n",
       "       [ 1.57398118e+02,  1.04895758e-01,  3.99020195e-02],\n",
       "       [-1.06551362e+01, -7.05160642e-03,  3.31509765e-02],\n",
       "       [ 1.11333928e+02,  7.32977335e-02,  3.16569303e-02],\n",
       "       [-6.76993656e+01, -4.54304741e-02,  3.70749150e-02],\n",
       "       [ 1.70824065e+02,  1.12504022e-01,  3.69077758e-02],\n",
       "       [ 3.74456359e+02,  2.49159522e-01,  3.05142292e-02],\n",
       "       [ 5.83937986e+02,  3.88600586e-01,  3.57977277e-02],\n",
       "       [ 2.52575424e+01,  1.74056945e-02,  3.58039891e-02],\n",
       "       [-8.41136467e+01, -5.55220217e-02,  2.94924188e-02]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load gumdrop_corr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy.random as ra\n",
    "import statistics \n",
    "from scipy import signal\n",
    "import numpy.linalg as la \n",
    "import HERA_hack\n",
    "\n",
    "#make sure that HERA hack is in the directory that you're running this code in! \n",
    "#Remember to SCP it over\n",
    "\n",
    "########### DEFINE OBS AND NECESSARY VARS #########\n",
    "\n",
    "\n",
    "freq_fid = 150\n",
    "\n",
    "dishes = np.array([[0,0],[0,55],[30,30],[0,60],[2,55],[47,2],[45,23],[56,21],[30,115],[48,52],[100,100],[0,200],[115,30],[33,31],[49,11],[21,24],[25,6],[56,9],[12,13],[16,17],[38,17],[60,14],[26,28],[6,45],[3,37],[12,55],[200,0],[145,13],[134,65],[139,163]])\n",
    "\n",
    "#observable corners of the sky [lat,long]\n",
    "acorner = np.array([[120,270],[122,280],[120,280],[122,270]])\n",
    "\n",
    "HERA = HERA_hack.telescope(dishes, latitude=-30, channel_width=1., Tsys=300, beam_width=3, beam = 'gaussian')\n",
    "\n",
    "obs = HERA_hack.observation(HERA, 100, 100, 0.01,acorner,1, 0.2, norm = False, pbeam = False)\n",
    "\n",
    "def generate_foregrounds():\n",
    "############ SYNCHRO EMISSION ############\n",
    "\n",
    "\talpha_0_syn = 2.8\n",
    "\tsigma_syn = 0.1\n",
    "\tAsyn = 335.4 #K\n",
    "\n",
    "\tpixel_flux_syn = []\n",
    "\n",
    "\talpha_syn = np.random.normal(alpha_0_syn,sigma_syn,obs.Npix)\n",
    "\n",
    "\tfor i in range(obs.Npix):\n",
    "\t    flux = Asyn*(obs.freq/freq_fid)**(-alpha_syn[i])\n",
    "\t    pixel_flux_syn.append(flux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t########### FREE FREE EMISSION ##########\n",
    "\n",
    "\talpha_0_ff = 2.15\n",
    "\tsigma_ff = 0.01\n",
    "\tAff = 33.5 #K\n",
    "\n",
    "\tpixel_flux_ff = []\n",
    "\n",
    "\talpha_ff = np.random.normal(alpha_0_ff,sigma_ff,obs.Npix)\n",
    "\n",
    "\tfor i in range(obs.Npix):\n",
    "\t    flux = Aff*(obs.freq/freq_fid)**(-alpha_ff[i])\n",
    "\t    pixel_flux_ff.append(flux)\n",
    "\n",
    "\t########### UNRES POINT SOURCE ###########\n",
    "\n",
    "\tgamma = 1.75\n",
    "\n",
    "\tdef dnds(s):\n",
    "\t    return 4.*(s/880)**(-gamma)\n",
    "\n",
    "\ts = np.arange(8,100,1) #maybe make this an argument \n",
    "\tn_sources = 10\n",
    "\n",
    "\tpdf = np.asarray([s,dnds(s)]) #0 is s, 1 is dnds\n",
    "\tprob = pdf[1]/float(sum(pdf[1]))\n",
    "\tcum_prob = np.cumsum(prob)\n",
    "\n",
    "\tdef gen_fluxes(N):\n",
    "\t    R = ra.uniform(0, 1, N)\n",
    "\t    #Here we first find the bin interval that random number lies in min(cum_prob[])\n",
    "\t    #then we find the flux who's index is that cum_prob\n",
    "\t    #repat for all r in R\n",
    "\t    return [int(s[np.argwhere(cum_prob == min(cum_prob[(cum_prob - r) > 0]))]) for r in R]\n",
    "\n",
    "\talpha_0 = 2.5\n",
    "\tsigma = 0.5\n",
    "\n",
    "\ttheta_res = np.abs(np.cos(obs.observable_coordinates()[1,0])-np.cos(obs.observable_coordinates()[0,0]))\n",
    "\tphi_res = obs.observable_coordinates()[30,1]- obs.observable_coordinates()[1,1]\n",
    "\tomega_pix = theta_res*phi_res\n",
    "\tfactor = 1.4e-6*((obs.freq/freq_fid)**(-2))*(omega_pix**(-1))\n",
    "\n",
    "\tpixel_flux = []\n",
    "\n",
    "\tfor i in range(obs.Npix):\n",
    "\t    alpha = np.random.normal(alpha_0,sigma,n_sources)\n",
    "\t    S_star = gen_fluxes(n_sources)\n",
    "\t    sum_fluxes = 0 \n",
    "\n",
    "\t    for i in range(n_sources-1):\n",
    "\t        sum_fluxes += factor*S_star[i]*(obs.freq/freq_fid)**(-alpha[i])\n",
    "\t    \n",
    "\t    pixel_flux.append(sum_fluxes/n_sources)\n",
    "\n",
    "\n",
    "\t########## TOTAL FG ################\n",
    "\n",
    "\tpixel_flux = np.asarray(pixel_flux)\n",
    "\tpixel_flux_ff = np.asarray(pixel_flux_ff)\n",
    "\tpixel_flux_syn = np.asarray(pixel_flux_syn)\n",
    "\n",
    "\ttotal_fg = pixel_flux + pixel_flux_ff + pixel_flux_syn\n",
    "\n",
    "\treturn total_fg\n",
    "\n",
    "\n",
    "\n",
    "############# X_CORR ############\n",
    "\n",
    "\n",
    "nreal = 3\n",
    "\n",
    "#cov_NFG = np.zeros(nreal)\n",
    "\n",
    "\n",
    "\n",
    "#cov_NN = np.zeros(nreal)\n",
    "\n",
    "#mean_prods_NFG = np.zeros(nreal)\n",
    "\n",
    "#sigma_prod_NFG = np.zeros(nreal)\n",
    "\n",
    "#mean_prods_NN = np.zeros(nreal)\n",
    "\n",
    "#sigma_prod_NN = np.zeros(nreal)\n",
    "\n",
    "corr_NFG = np.zeros(nreal)\n",
    "\n",
    "corr_SN = np.zeros(nreal)\n",
    "\n",
    "corr_NN = np.zeros(nreal)\n",
    "\n",
    "    \n",
    "for i in range(nreal):\n",
    "    noise_1 = np.real(obs.generate_map_noise())\n",
    "    noise_2 = np.real(obs.generate_map_noise())\n",
    "    fg_1= generate_foregrounds()\n",
    "    fg_2 = generate_foregrounds()\n",
    "    n1 = noise_1 - np.mean(noise_1)\n",
    "    n2 = noise_2 - np.mean(noise_2)\n",
    "    nfg_1 = n1 + (fg_1 - np.mean(fg_1))\n",
    "    nfg_2 = n2+ (fg_2 - np.mean(fg_2))\n",
    "\n",
    "    norm_1 = np.sqrt(np.dot(n1,n1))\n",
    "    norm_2 = np.sqrt(np.dot(n2,n2))\n",
    "    norm_nfg_1 = np.sqrt(np.dot(nfg_1,nfg_1))\n",
    "    norm_nfg_2 = np.sqrt(np.dot(nfg_2,nfg_2))\n",
    "    \n",
    "    cov_NFG = np.dot(nfg_1,nfg_2)\n",
    "    corr_NFG[i] = cov_NFG /(norm_nfg_1*norm_nfg_2)\n",
    "\n",
    "    cov_NN = np.dot(n1,n2)\n",
    "    corr_NN[i] = cov_NN /(norm_1*norm_2)\n",
    "\n",
    "    cov_SN = np.dot(n1,n1)\n",
    "    corr_SN[i] = cov_SN /(norm_1*norm_1)\n",
    "    \n",
    "#     noise_1 = np.real(obs.generate_map_noise())\n",
    "#     noise_2 = np.real(obs.generate_map_noise())\n",
    "#     fg_1= generate_foregrounds()\n",
    "#     fg_2 = generate_foregrounds()\n",
    "    \n",
    "#     print(fg_1)\n",
    "#     print(fg_2)\n",
    "#     print(noise_1)\n",
    "#     print(noise_2)\n",
    "\n",
    "#     nfg_1 = noise_1 + fg_1\n",
    "#     nfg_2 = noise_2 + fg_2\n",
    "    \n",
    "#     print(nfg_1)\n",
    "#     print(nfg_2)\n",
    "#     assert False\n",
    "\n",
    "#     norm_1 = np.sqrt(np.dot(noise_1,noise_1))\n",
    "#     norm_2 = np.sqrt(np.dot(noise_2,noise_2))\n",
    "#     norm_nfg_1 = np.sqrt(np.dot(nfg_1,nfg_1))\n",
    "#     norm_nfg_2 = np.sqrt(np.dot(nfg_2,nfg_2))\n",
    "    \n",
    "#     cov_NFG = np.dot(nfg_1,nfg_2)\n",
    "#     mean_prods_NFG = (np.mean(nfg_1))*(np.mean(nfg_2))\n",
    "#     corr_NFG[i] = (cov_NFG - (mean_prods_NFG))/(norm_nfg_1*norm_nfg_2)\n",
    "    \n",
    "#     cov_NN = np.dot(noise_1,noise_2)\n",
    "#     mean_prods_NN = (np.mean(noise_1))*(np.mean(noise_2))\n",
    "#     corr_NN[i] = (cov_NN - (mean_prods_NN))/(norm_1*norm_2)\n",
    "    \n",
    "#     cov_SN = np.dot(noise_1,noise_1)\n",
    "#     mean_prods_SN = (np.mean(noise_1))*(np.mean(noise_1))\n",
    "#     corr_SN[i] = (cov_SN - (mean_prods_SN))/(norm_1*norm_1)\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     cov_NFG[i] = np.dot(noise_1,fg)\n",
    "#     mean_prods_NFG[i] = (np.mean(noise_1))*(np.mean(fg))\n",
    "#     sigma_prod_NFG[i] = (statistics.stdev(noise_1))*(statistics.stdev(fg))\n",
    "\n",
    "#     cov_NN[i] = np.dot(noise_1,noise_2)\n",
    "#     mean_prods_NN[i] = (np.mean(noise_1))*(np.mean(noise_2))\n",
    "#     sigma_prod_NN[i] = (statistics.stdev(noise_1))*(statistics.stdev(noise_2))\n",
    "\n",
    "# stacked_NFG = np.reshape((np.dstack((cov_NFG,mean_prods_NFG,sigma_prod_NFG))),(nreal,3))\n",
    "# # stacked_NN = np.reshape((np.dstack((cov_NN,mean_prods_NN,sigma_prod_NN))),(nreal,3))\n",
    "\n",
    "\n",
    "# #np.savetxt('noise_fg.txt',stacked_NN)\n",
    "\n",
    "# np.savetxt('same_noise.txt', stacked_NFG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2*np.ones(10)\n",
    "y = np.arange(-10,10,1)\n",
    "\n",
    "x-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9978624  0.99791029 0.99795892]\n"
     ]
    }
   ],
   "source": [
    "print(corr_NFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04363303, 0.04780113, 0.02240051])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01521259,  0.002039  , -0.00917932])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_NFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37workshop",
   "language": "python",
   "name": "p37workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
